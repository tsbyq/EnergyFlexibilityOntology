{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_window': {'start': '2019-01-01T00:00:00Z',\n",
       "  'end': '2019-01-02T00:00:00Z'},\n",
       " 'peak_timestamps': {'manual_input': True,\n",
       "  'values': ['2019-01-01T12:00:00Z', '2019-01-01T13:00:00Z']},\n",
       " 'data_source_type': 'csv',\n",
       " 'path_to_dataset': 'demo_validation_data.csv',\n",
       " 'data_reading_method': 'pandas',\n",
       " 'pandas_parsing_specs': {'timestamps': {'column_identifier': 'Datetime',\n",
       "   'quantity': 'time',\n",
       "   'dtype': 'datetime',\n",
       "   'unit': None},\n",
       "  'baseline_power_profile': {'column_identifier': 'Baseline (kW)',\n",
       "   'quantity': 'power',\n",
       "   'dtype': 'float',\n",
       "   'unit': 'kW'},\n",
       "  'flexible_power_profile': {'column_identifier': 'Flexible (kW)',\n",
       "   'quantity': 'power',\n",
       "   'dtype': 'float',\n",
       "   'unit': 'kW'},\n",
       "  'baseline_energy_profile': {'column_identifier': 'Baseline (kWh)',\n",
       "   'quantity': 'energy',\n",
       "   'dtype': 'float',\n",
       "   'unit': 'kWh'},\n",
       "  'flexible_energy_profile': {'column_identifier': 'Flexible (kWh)',\n",
       "   'quantity': 'energy',\n",
       "   'dtype': 'float',\n",
       "   'unit': 'kWh'},\n",
       "  'generic_quantity_profile': {'column_identifier': 'Cost ($)',\n",
       "   'quantity': None,\n",
       "   'dtype': 'float',\n",
       "   'unit': None},\n",
       "  'price_signal': {'column_identifier': 'Price ($/kWh)',\n",
       "   'quantity': 'cost / energy',\n",
       "   'dtype': 'float',\n",
       "   'unit': '$/kWh'},\n",
       "  'emission_factor': {'column_identifier': 'Emission Factor (kgCO2e/kWh)',\n",
       "   'quantity': 'mass / energy',\n",
       "   'dtype': 'float',\n",
       "   'unit': 'kgCO2e/kWh'}},\n",
       " 'other_parsing_specs': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path to the YAML configuration file\n",
    "config_file_path = 'demo_config.yml'  # Replace with actual path\n",
    "\n",
    "# Function to read a YAML file and return a dictionary\n",
    "def read_yaml_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config_dict = yaml.safe_load(file)\n",
    "    return config_dict\n",
    "\n",
    "# Reading the configuration file\n",
    "config_data = read_yaml_config(config_file_path)\n",
    "config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config_data['path_to_dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Baseline (kWh)</th>\n",
       "      <th>Flexible (kWh)</th>\n",
       "      <th>Peak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/17/2021 0:00</td>\n",
       "      <td>0.030926</td>\n",
       "      <td>-3.620000e-14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/17/2021 0:01</td>\n",
       "      <td>0.031567</td>\n",
       "      <td>2.610000e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/17/2021 0:02</td>\n",
       "      <td>0.031963</td>\n",
       "      <td>2.650000e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/17/2021 0:03</td>\n",
       "      <td>0.032306</td>\n",
       "      <td>2.690000e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/17/2021 0:04</td>\n",
       "      <td>0.032656</td>\n",
       "      <td>2.720000e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>12/21/2021 23:55</td>\n",
       "      <td>0.033474</td>\n",
       "      <td>1.021433e-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>12/21/2021 23:56</td>\n",
       "      <td>0.033610</td>\n",
       "      <td>1.021549e-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7197</th>\n",
       "      <td>12/21/2021 23:57</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>1.021675e-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>12/21/2021 23:58</td>\n",
       "      <td>0.033582</td>\n",
       "      <td>1.021811e-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>12/21/2021 23:59</td>\n",
       "      <td>0.033645</td>\n",
       "      <td>1.021958e-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime  Baseline (kWh)  Flexible (kWh)   Peak\n",
       "0      12/17/2021 0:00        0.030926   -3.620000e-14  False\n",
       "1      12/17/2021 0:01        0.031567    2.610000e-11  False\n",
       "2      12/17/2021 0:02        0.031963    2.650000e-11  False\n",
       "3      12/17/2021 0:03        0.032306    2.690000e-11  False\n",
       "4      12/17/2021 0:04        0.032656    2.720000e-11  False\n",
       "...                ...             ...             ...    ...\n",
       "7195  12/21/2021 23:55        0.033474    1.021433e-02  False\n",
       "7196  12/21/2021 23:56        0.033610    1.021549e-02  False\n",
       "7197  12/21/2021 23:57        0.033553    1.021675e-02  False\n",
       "7198  12/21/2021 23:58        0.033582    1.021811e-02  False\n",
       "7199  12/21/2021 23:59        0.033645    1.021958e-02  False\n",
       "\n",
       "[7200 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from pyshacl import validate\n",
    "\n",
    "# Load the data graph (assuming you have a function to do this based on your config file)\n",
    "data_graph = create_data_graph_from_config('demo_config.yml')\n",
    "\n",
    "# Load the SHACL shapes graph\n",
    "shapes_graph = rdflib.Graph()\n",
    "shapes_graph.parse('path/to/shapes_file.ttl', format='ttl')\n",
    "\n",
    "# Run SHACL validation\n",
    "conforms, results_graph, results_text = validate(data_graph, shacl_graph=shapes_graph)\n",
    "\n",
    "# Check if the data conforms to the SHACL shapes\n",
    "if conforms:\n",
    "    print(\"The dataset is sufficient for the Flexibility Factor KPI calculation.\")\n",
    "else:\n",
    "    print(\"The dataset does not meet the requirements for the Flexibility Factor KPI:\")\n",
    "    print(results_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an RDF graph from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@prefix ns1: <http://example.org/efkpis#> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\nns1:BaselineEnergyProfile ns1:column_identifier \"Baseline (kWh)\" ;\\n    ns1:dtype \"float\" ;\\n    ns1:quantity \"energy\" ;\\n    ns1:unit \"kWh\" .\\n\\nns1:EvaluationWindow ns1:endTime \"2019-01-02T00:00:00+00:00\"^^xsd:dateTime ;\\n    ns1:startTime \"2019-01-01T00:00:00+00:00\"^^xsd:dateTime .\\n\\nns1:FlexibleEnergyProfile ns1:column_identifier \"Flexible (kWh)\" ;\\n    ns1:dtype \"float\" ;\\n    ns1:quantity \"energy\" ;\\n    ns1:unit \"kWh\" .\\n\\nns1:GenericLoadProfile ns1:column_identifier \"Cost ($)\" ;\\n    ns1:dtype \"float\" ;\\n    ns1:quantity \"unknown\" ;\\n    ns1:unit \"unknown\" .\\n\\nns1:HighLoadEndTimestamp ns1:hasValue \"2019-01-01T13:00:00+00:00\"^^xsd:dateTime .\\n\\nns1:HighLoadStartTimestamp ns1:hasValue \"2019-01-01T12:00:00+00:00\"^^xsd:dateTime .\\n\\n[] ns1:dataReadingMethod \"pandas\" ;\\n    ns1:dataSourceType \"csv\" ;\\n    ns1:pathToDataset \"demo_validation_data.csv\" .\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rdflib\n",
    "import yaml\n",
    "\n",
    "def create_data_graph_from_config(config_file_path):\n",
    "    # Load the config file\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # Initialize an RDF graph\n",
    "    g = rdflib.Graph()\n",
    "\n",
    "    # Namespace for your ontology\n",
    "    efkpis = rdflib.Namespace(\"http://example.org/efkpis#\")\n",
    "    xsd = rdflib.Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
    "\n",
    "    # Add the necessary triples based on the config file\n",
    "    # ------------------- Evaluation Window -------------------\n",
    "    evaluation_window = efkpis.EvaluationWindow\n",
    "    start_time = rdflib.Literal(config['evaluation_window']['start'], datatype=xsd.dateTime)\n",
    "    end_time = rdflib.Literal(config['evaluation_window']['end'], datatype=xsd.dateTime)\n",
    "    g.add((evaluation_window, efkpis.startTime, start_time))\n",
    "    g.add((evaluation_window, efkpis.endTime, end_time))\n",
    "\n",
    "\n",
    "    # ------------------- Data Source Information -------------------\n",
    "    # Add data source type, path, and reading method to the graph\n",
    "    data_source_type = rdflib.Literal(config['data_source_type'])\n",
    "    path_to_dataset = rdflib.Literal(config['path_to_dataset'])\n",
    "    data_reading_method = rdflib.Literal(config['data_reading_method'])\n",
    "\n",
    "    # Assign a unique identifier for the data source information\n",
    "    data_source_info = rdflib.BNode()\n",
    "    g.add((data_source_info, efkpis.dataSourceType, data_source_type))\n",
    "    g.add((data_source_info, efkpis.pathToDataset, path_to_dataset))\n",
    "    g.add((data_source_info, efkpis.dataReadingMethod, data_reading_method))\n",
    "\n",
    "\n",
    "    # ------------------- Peak Timestamps Specs -------------------\n",
    "    # High Load Start and End Timestamps\n",
    "    for ts in config['peak_timestamps']['values']:\n",
    "        timestamp = rdflib.Literal(ts, datatype=xsd.dateTime)\n",
    "        if ts == config['peak_timestamps']['values'][0]:\n",
    "            g.add((efkpis.HighLoadStartTimestamp, efkpis.hasValue, timestamp))\n",
    "        else:\n",
    "            g.add((efkpis.HighLoadEndTimestamp, efkpis.hasValue, timestamp))\n",
    "\n",
    "    # ------------------- Load Profiles Specs -------------------\n",
    "    # Common function to add profiles to the graph\n",
    "    def add_profile_to_graph(profile_name, profile_data):\n",
    "        profile_node = efkpis[profile_name]\n",
    "        for key, value in profile_data.items():\n",
    "            literal_value = rdflib.Literal(value or \"unknown\")\n",
    "            g.add((profile_node, efkpis[key], literal_value))\n",
    "\n",
    "    # Add profiles to the graph\n",
    "    add_profile_to_graph('GenericLoadProfile', config['pandas_parsing_specs']['generic_quantity_profile'])\n",
    "    add_profile_to_graph('BaselineEnergyProfile', config['pandas_parsing_specs']['baseline_energy_profile'])\n",
    "    add_profile_to_graph('FlexibleEnergyProfile', config['pandas_parsing_specs']['flexible_energy_profile'])\n",
    "\n",
    "    # Return the RDF graph\n",
    "    return g\n",
    "\n",
    "# Example usage\n",
    "data_graph = create_data_graph_from_config('demo_config.yml')\n",
    "\n",
    "data_graph.serialize(format='turtle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query evaluation window parameters from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2019-01-01T00:00:00+00:00, End Time: 2019-01-02T00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# SPARQL query to get the evaluation window\n",
    "query = \"\"\"\n",
    "PREFIX efkpis: <http://example.org/efkpis#>\n",
    "\n",
    "SELECT ?startTime ?endTime\n",
    "WHERE {\n",
    "    efkpis:EvaluationWindow efkpis:startTime ?startTime ;\n",
    "                           efkpis:endTime ?endTime .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "qres = data_graph.query(query)\n",
    "\n",
    "# Print results\n",
    "for row in qres:\n",
    "    print(f\"Start Time: {row.startTime}, End Time: {row.endTime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query high load start and end timestamp from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Load Start Timestamp: 2019-01-01T12:00:00+00:00, High Load End Timestamp: 2019-01-01T13:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# SPARQL query to get the high load timestamps\n",
    "query = \"\"\"\n",
    "PREFIX efkpis: <http://example.org/efkpis#>\n",
    "\n",
    "SELECT ?startTimestamp ?endTimestamp\n",
    "WHERE {\n",
    "    efkpis:HighLoadStartTimestamp efkpis:hasValue ?startTimestamp .\n",
    "    efkpis:HighLoadEndTimestamp efkpis:hasValue ?endTimestamp .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "qres = data_graph.query(query)\n",
    "\n",
    "# Print results\n",
    "for row in qres:\n",
    "    print(f\"High Load Start Timestamp: {row.startTimestamp}, High Load End Timestamp: {row.endTimestamp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query load profile data specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SPARQL query to get GenericLoadProfile specs\n",
    "query = \"\"\"\n",
    "PREFIX efkpis: <http://example.org/efkpis#>\n",
    "\n",
    "SELECT ?columnIdentifier ?quantity ?dtype ?unit\n",
    "WHERE {\n",
    "    efkpis:GenericLoadProfile efkpis:columnIdentifier ?columnIdentifier ;\n",
    "                              efkpis:quantity ?quantity ;\n",
    "                              efkpis:dataType ?dtype ;\n",
    "                              efkpis:unit ?unit .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query on your RDF graph\n",
    "qres = data_graph.query(query)\n",
    "\n",
    "# Print results\n",
    "for row in qres:\n",
    "    print(f\"Column Identifier: {row.columnIdentifier}, Quantity: {row.quantity}, Data Type: {row.dtype}, Unit: {row.unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JPTR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
